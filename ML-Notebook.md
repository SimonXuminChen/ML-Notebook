# ML Notebook

## 1. Feature Engineering 特征工程

### 1.1 特征选择

#### 1.1.1如何选择特征？

基于业务理解，尽可能找出对因变量有影响的所有自变量。



#### 1.1.2如何评估特征的可用性？

从三个方面去评估：特征数据的获取难度、覆盖率、准确率。



### 1.2 特征处理

未经过处理的特征可能会出现以下情况：

- 不同量级
- 离散型特征
- 存在缺失值
- 信息利用率低

#### 1.2.1数据清洗

**To-Do:待补充**



#### 1.2.2 标准化、归一化、中心化

对数值类型的特征作标准化，可以将所有的特征统一到一个大致相同的数值区域。

原因是相近取值范围的特征，在进行优化的时候，能够更快速地找到最优解。

归一化是指将范围缩到[0,1]

常用的方法有：

- z-score

  将两组或多组数据转化为无单位的Z-Score分值，使得数据标准统一化，提高了数据可比性，削弱了数据解释性。

  $$x_{new} = \frac {x - \mu}{\sigma}$$

- Min-Max 

  $$x_{new} = \frac {x-x_{min}}{x_{max}-x_{min}}$$

- decimal scaling

  $$x_{new}=\frac {x}{10^j}$$

归一化不是万能的，通过梯度下降发求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但是对于决策树模型是不适用的。



#### 1.2.3 

#### 1.2.4离散型特征的处理方式

许多模型和算法都需要数值特征作为输入，因此需要将离散型的特征转换为数值特征。

离散型特征转换数值特征的方法：

- 序列编码
- one-hot编码
- 二进制编码



##### 序列编码

序列特征通常用来处理具有大小关系的特征。

如果特征中是可以被等级划分的，则可以划分成高中低等级，通过使用序列编码，将低等级用1表示，中等级用2表示，高等级用3表示。

##### one-hot编码

根据特征的数量，将每个特征编码成1*N维的向量，只有代表这个特征的维度表示为1，其他维度表示为0。

会具有**维度爆炸**的问题。

##### 二进制编码

将特征通过过序列标注，然后以二进制的形式，将特征序列号转换为二进制，这样可以减少空间。



#### 1.2.5缺失值的处理

1. 通过平均值、中值、众数、随机值等替代。效果一般，因为这相当于人为添加了噪声。
2. 通过欧式距离或者Pearson相似度，来确定与缺失数据样本最近的K个样本， 将这K个样本的相关特征加权平均来估计该样本的缺失值。
3. 将变量映射到高维空间
   - 离散型变量：通过one-hot编码，映射成三个变量：男，女，缺失三个值向量
   - 连续性变量：进行分箱操作，采用一定的数据平滑方式（平均值/中值/箱边界）进行离散化，然后增加是否缺失这种维度。
4. 通过已有数据作线性回归，预测缺失值。



#### 1.2.6离群值outliers的处理

因为过大或过小的数据可能会影响到分析结果，尤其是在做回归的时候，我们需要对那些离群值进行处理。

> 离群值和极值是不一样的，极值不一定会影响分析结果。

检测方法：

1. 基于标准差（Standard2Deviations, SD）法
   a为一组数的平均数，b为这组数的标准差，则超出[a-2b, a+2b]范围的值即被认为离群值。

   

   这种方法不是特别的靠谱，因为离群值的出现可能反过来很大程度影响平均数和标准差，所以平均数或者标准差受离群值的制约而使得这个检测方法不靠谱。

2. 基于绝对离差中位数（Median Absolute Deviation, MAD）的中心距离计算法
    （1）计算所有观察点的中位数median(X);
    （2）计算每个观察点与中位数的绝对偏差值abs(X-median(X));
    （3）计算（2）中的绝对偏差值的中位数，即MAD = median(abs(X - median(X)));
    （4）之后选定n值，从而确定合理的范围

  $$X_i^{'} = \begin{cases}  
  X_{median}+nMAD & X_i>X_{median}+nMAD \\
  X_{median}-nMAD & X_i<X_{median}-nMAD \\
  X_i & X_{median}-nMAD<X_i<X_{median}+nMAD
  \end{cases}$$

  这是一种稳健对抗离群数据的距离值方法，采用计算各观测值与平均值的距离总和的方法。放大了离群值的影响，相比基于SD的中位数距离法，它可以更清晰地从正常观察点中检测出离群值来。

3. 百分位法

   计算的逻辑是将因子值进行升序的排序，对排位百分位高于97.5%或排位百分位低于2.5%的因子值，进行类似于前两种方法的处理。

4. Box plot

另外，在RMSE中，outliers会给模型评估造成很大的影响，所以要在预处理阶段过滤掉outliers，或者进一步提高模型的预测能力，也可以根管一个合适的评估指标，如MAPE(Mean Absolute Percent Error平均绝对百分比误差)，计算公式为$$MAPE = \sum_{i=1}^n |\frac {y_i-y_i^{hat}}{y_i}|*\frac {100}{n}$$

MAPE将每个样本的误差进行了归一化，降低了个别outliers带来的绝对误差的影响。



#### 1.2.7特征组合

有时候为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

##### 组合方法

- A*A
- A*B
- A\*B\*C*D

##### 处理高维特征

有时候，特征的数量过大（可以达到千万量级），这时候几乎是无法学习到这种规模的参数，因此可以采用矩阵分解的方法将特征用低维向量的方法表示。

如项目中的DeepFM、FM，采用了因子分解机

##### 如何选择组合特征

可以采用GBDT（梯度提升决策树）的方法，每次都在之前构建的决策树的残差上构建下一棵决策树。



#### 1.2.8特征选择

数据预处理结束之后，我们要选择有意义的特征去输入算法和模型进行训练，通常从以下两个方面来考虑：

- 是否发散
- 与目标的相关性



特征的选择方法可以分为Filter法（过滤法）、Wrapper法（包装法）、Embedded法（嵌入法）

##### Filter法-过滤法

按照发散性或者相关性，对各个特征进行评分，设定阈值或者选择阈值的个数，选择特征。

- 方差选择法：先计算各个特征的方差，选择方差大于阈值的特征。

- 相关系数法：先计算各个特征对目标值的相关系数P值。

- 卡方检验(Chi-square test)：检验定性自变量对定性因变量的相关性。$$x^2 = \sum \frac {(Observed - Expect)^2}{Expect}$$

  结合自由度、置信度查表分析。

  _reference-《结合日常生活的例子，了解什么是卡方检验》:https://www.jianshu.com/p/807b2c2bfd9b_

  

##### Wrapper法-包装法

根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。

- 递归特征消除法：使用一个及模型来进行多轮训练，每轮训练后，消除若干权值系数的特征

##### Embedded法-嵌入法

先试用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是通过训练来确定特征的优劣。

- 基于惩罚项的特征选择法：使用带有惩罚项的基模型，除了筛选出特征外，同时也进行了降维
- 基于树模型的特征选择法(GBDT)



#### 1.2.9降维处理

当特征选择完成后，如果特征矩阵过大，会导致计算量大、训练时间长，所以为了有效、快速的训练模型，降低特征矩阵维度是一个有效的手段。



常见的降维方法除了基于L1惩罚项的模型外，还有以下两种方法：

- PCA（主成分分析）
- LDA（线性判别分析）

> 线性判别分析本身也是一个分类模型

PCA和LDA有很多共同点，本质都是要将原始的样本映射到维度更低的样本空间。但是两者的映射目标不一样，PCA是为了让映射后的样本具有最大发散性；而LDA是为了让映射后的样本有最好的分类性能。

所以，PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。

PCA与LDA的区别：

1. PCA选择的是投影后数据方差最大的方向，因为PCA是无监督的，所以PCA假设方差越大，信息量越多，用主成分来表示数据可以去除冗余的维度，达到降维，而LDA选择的是类内方差小，类间方差打的方向。
2. 另外LDA用到了类别标签信息，为了找到数据中具有判别性的维度，使得这些原始数据在这些方向上投影后，不同类别尽可能区分开。

##### PCA

- 最大方差
- 最小平方误差

最大方差理论计算方法：

1. 对样本数据进行中心化处理

2. 求样本协方差矩阵

3. 对协方差矩阵进行特征值分解，将特征值从大到小排列。

4. 去特征值前d大对应的特征向量$w_1,w_2,...,w_d$,将n维样本映射到d维

   

##### LDA

- 最大化类间距离
- 最小化类内距离





## 2. 模型评估

在机器学习领域，模型的评估也至关重要，只有选择与问题相匹配的评估方法，才能快速发现模型选择或训练过程中出现的问题，迭代地对模型进行优化。如果不能合理的运用评估指标，不仅不能发现模型的问题，还会得出错误的结论。



模型评估主要分为两个阶段：离线评估和在线评估。

离线评估：模型未部署到生产环境之前。首先需要训练一个模型，然后对训练好的模型进行离线评估来了解下模型的性能情况。



线上评估：模型部署到生产环境之后。采用训练好的模型。



>  不同的模型有不同的评估指标



先说一下几个通常使用的标准：

**准确率Accuracy：**指分类正确的样本站样本个数的比例，是最简单也是最直观的评价指标。公式如下：

$$Accuracy = \frac {n_{correct}} {n_{total}} $$

但是准确率有一个明显的缺陷，当数据样本不均衡的时候，占比重较大的类别会成为影响准确率的最主要因素。所以可以通过**平均准确率**（每个类别下的样本准确率的算术平均）作为模型评估的指标。



**精准率Precision：**指分类正确的正样本个数占分类器判定为正样本的样本个数比例。

$$Precision = \frac {n_{predicted true & actual true} }{n_{predicted true}}$$

**召回率Recall：**指分类正确的正样本个数占真正正样本个数的比例。

$$Recall = \frac {n_{predicted true}} {n_{actual true}}$$



**F1-score：**：是精准率和召回率的调和平均值

$$F1 = \frac {2\*precision\*recall}{precision+recall}$$

Precision和Recall是及矛盾又统一的两个指标，为了提高Precision值，分类器需要尽量在“更有把握”时，才把样本预测为正样本，但是此时会往往因为过于保守而漏掉很多“没有把握”的正样本，导致Recall降低。



> 在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用TopN返回结果的Precision和Recall来衡量排序模型的性能，即认为模型返回的TopN结果就是模型判定的正样本，然后计算前N个位置上的准确率Precision@N和召回率Recall@N.



为了综合一个模型的好坏，不仅要看Precision@N和Recall@N，而且最好绘制出模型的**P-R曲线（Precision-Recall Curve）**

### 2.1 Precision-Recall Curve

P-R曲线的很轴是召回率，纵轴是精确率。P-R曲线上的一个点代表在某一阈值下，模型将大于该阈值的结果判定为正样本，小于的判定为负样本，此时的返回结果对应的召回率和精确率。

整条P-R曲线是通过将阈值从高到低移动而生成的，原点附近代表当阈值最大时，模型的精确率和召回率

**To-Do:补充图片**



最重要的是，不能只通过某个点对应的精确率和召回率去全面衡量模型的性能，这是不准确的，只有通过P-R曲线的整体表现，才能对模型进行更为全面的评估。



**MAPE**

之前提到过。



##### Confusion Matrix





### 2.2 ROC & AUC

#### 2.2.1 ROC(Receiver Operation Characteristic Curve 受试者工作特征曲线)

TAT这名字真难记。

ROC的横轴为假阳性率（False Positive Rate, FPR）,纵轴为真阳性率（True Positive Rate,TPR）。

$$ FPR = \frac {FP}{N}$$

$$TPR = \frac{TP}{P}$$

$$注意：N不是样本总个数，而是真实的负样本的数目；P是真实的正样本数目$$

ROC曲线一般都处于 $y=x$这条直线的上方，如果不是，只需要通过翻转模型预测的概率成1-p即可得到一个更好的分类器。

#### 2.2.2 如何绘制ROC

ROC曲线是通过不断移动分类器的“截断点”来生成曲线上的一组关键点的。

绘制方法一：

1. 首先根据样本标签统计出正负样本的数量，假设正样本数量为P，负样本数量为N；
2. 接下来把横轴的刻度间隔设置为1/N，纵轴的刻度间隔设置为1/P；
3. 再根据模型输出的预测概率对样本进行排序（从高到低）
4. 依次遍历样本，同事从零开始绘制ROC曲线，没遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，没遇到负样本就沿横轴方向绘制一个刻度间隔的曲线，知道遍历完所有的样本；
5. 曲线最终停留在 (1,1) 这个点，整个ROC曲线绘制完成。



绘制方法二：

1. 将样本按照预测概率从高到低排序，在输出最终的正负样本之前，设定阈值，大于阈值的为正例，小于阈值的为负例
2. 通过动态地调整截断点（阈值），从正无穷开始，逐渐减小阈值到0，每一个截断点都会对应一个FPR和TPR
3. 在ROC图上绘制出每个截断点对应的位置
4. 连接所有点得到ROC曲线



#### 2.2.3 AUC（Area Under the Curve）

顾名思义，就是ROC曲线下的面积，AUC可以量化地反应基于ROC曲线衡量出的模型性能。

**AUC计算：**沿着ROC横轴方向做积分。取值一般在0.5~1之间。

> AUC的值越大，说明分类器月可能吧真正的正样本排在前面，分类性能越好。



#### 2.2.4 ROC和P-R Curve的区别

当正负样本的分布发生变化的时候，P-R Curve会发生剧烈的变化，而ROC的形状基本保持不变，即ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。

但是直观上，P-R Curve更能反映模型在特定数据集的性能。



### 2.3 距离计算

在模型的训练过程中，也需要不断评估样本之间的相似性（距离）。

而距离与相似度的关系为:$Distance = 1- Similarity$

机器学习问题中，通常将特征表示为向量的形式，所以在分析两个特征向量之间的相似性时，常用余弦相似度表示。

余弦相似度的取值范围[-1,1]，相同的两个向量之间的相似度为1。余弦距离的取值范围是[0,2]，相同的两个向量的余弦距离为0。



另外的距离和相似度方法有

距离：

- Minkowski距离

- 欧式距离
- 余弦距离



相似度：

- Pearson coefficient
- 余弦相似度



#### 2.3.1 距离定义

**定义：**在一个集合中，如果每一对元素均可唯一确定一个实数，使得三条距离公理（正定性，对称性，三角不等式）成立，则该实数可称为这对元素之间的距离。

正定性：大于0

对称性：dist(A,B)=dist(B,A)

三角不等式：a+b>c



#### 2.3.2 余弦相似度

$ cos(A,B)= \frac {A·B}{||A||_2||B||_2}$

  取两个向量夹角的余弦，关注的是向量之间的角度关系，并不关心值的大小。

**余弦相似度和欧氏距离的关系**

在维度高的时候，余弦相似度依然保持“相同为1，正交时为0，相反时为-1”，而欧式距离则受严重的影响，范围不固定。

> 欧氏距离体现的是数值上的绝对差异，而余弦距离体现方向上的相对差异。

如果向量的模长是讲过归一化处理，欧氏距离与余弦距离有着单点关系：$$||A-B||_2 = \sqrt {2(1-cos(A,B))}$$





注意：根据距离的定义，余弦距离并不是一个严格定义的距离，因为并不满足三角不等式这一点，这一点可以通过(0,1),(1,0),(1,1)这三个向量来距离证明，或者通过余弦距离和欧式距离的关系来证明。

此外，同样不属于严格定义的距离还有KL散度（KL距离）。

- KL散度常用于计算两个分部之间的差异。

**TO-Do:补充KL散度公式**

##### TO-DO:补充常用距离



### 2.4 A/B Test

在机器学习中，A/B Test是验证模型最终效果的主要手段，是为同一个目标制定的两个方案，在同一时间维度，分别让组成成分相同或相似的用户群组随机的使用一个方案，收集各群主的用户体验数据和业务数据，最后根据显著性分析评估出最好版本正式采用。

测试目的：页面（版本）的某一特定更新对目的指标的影响效果。

简单来讲，A/B Test是用来对产品的两个版本进行比较，将用户随机分成两组，这两组数据可以来自同一分布。

一组叫做对照组，用原始版本，另一组叫做实验组，使用新版。

**同时做线上测试，然后采集指标，分析结果。**



#### 2.4.1 线上A/B Test的原因

1. 离线评估无法完全消除模型过拟合的影响。
2. 离线评估无法还原线上的工程环境，考虑不到实际的情况，如数据分布，数据丢失，标签数据缺失等。
3. 某些业务指标在离线评估中无法计算。离线评估一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标往往无法直接获得。

#### 2.4.2 如何进行线上A/B Test

主要手段是对用户进行分桶，根据对照组和实验组分用户，分桶的过程中需要注意样本的独立性和采样方式的无偏性，确保同一个用户只能分到一个桶中，在分桶过程中，user_id需要是随机数，这样才能保证桶中的样本是无偏的。



### 2.5 模型评估方法

机器学习中，通常将样本分为训练集和测试集，训练集用于训练模型，使模型学习到参数，测试集用于评估模型。

#### 2.5.1主要的验证方法

1. Holdout 验证

   最简单的验证方法，将原数据集直接分成训练集和验证集，一般是训练集70%，验证集30%。但是该验证方法与分组有关，导致结果会出现随机性。

2. 交叉验证 Cross-Validation

   对Holdout验证法的改进。

   k-fold：交叉验证的一种，首先将样本分为k个相等大小的样本子集，依次遍历这些子集。遍历时，把当前子集作为验证集，其余子集作为训练集，进行模型的训练和评估。最后把k次评估指标的**平均值**作为最终的评估指标。

   留p验证：每次都留下p个样本作为验证集（从n个元素中选择p个元素,总共$C_n^p$种可能。开销大。

   leave-one-out: 留一法，留p验证的特例，每次留下一个样本作为验证集，其余样本作为测试集。同样是遍历所有的样本，并取平均值作为最后评估指标。**样本数量大的时候，留一法的时间开销大。**

   

3. 自助法

   总共有n个样本，每次都从样本中有放回的随机抽取一个，直到训练集的数量达到n个为止，其中有可能抽取到相同的样本，将剩下未抽到的样本作为验证集。

   自助法是在样本数量不足的情况下提出的，因为样本数量少，如果采用Holdout或cross-validation的话， 会导致样本数量更加稀少。

   当n趋向于无穷大的时候，大概有0.368左右的数据未被选择过。（通过重要极限计算）



### 2.6 超参数调优

调参侠来了！

除了根据经验设定的所谓的合理值之外，一般很难找到合理的方法去寻找超参数的最优取值。

优化目标：

1. 目标函数（损失函数），即算法需要最大化或最小化的目标
2. 搜索范围，一般通过上限和下限决定
3. 其他参数，比如搜索步长



为了进行超参数调优，一般采用以下的方法：

- 网格搜索
- 随机搜索
- 贝叶斯优化



##### 网格搜索

通过查找搜索范围内的所有的点来确定最优值。

如果采用较大的搜索范围和较小的步长，那么很大概率找到全局最优值，但是这样的计算开销十分大，尤其是需要调参的超参数比较多的时候。

所以实际上，先采用**较广的搜索范围和较大的步长**去寻找到最有可能存在最优解的位置，然后逐渐**缩小搜索范围和步长**去寻找更精确的最优值。这样可以降低计算量和时间，但是由于目标函数一般是**非凸**的，所以很可能错过全局最优解。



##### 随机搜索

与网格搜索相似，不过不像网格搜索那样在范围内全部搜索，而是通过随机选取。思想是如果数据集足够大，那么通过随机采样也能大概率地找到全局最优值或者近似值。

优点：速度快

**缺点：结果不敢保证**



> 网格搜索和随机搜索测试一个新数据的时候，会忽略之前那个点的信息。



##### 贝叶斯优化

通过对目标函数的形状进行学习，找到使目标函数向全局最优解逼近的参数。

**贝叶斯优化会利用之前的样本信息。**

方法：

1. 首先根据先验分布，假设一个搜集函数
2. 每次使用新的样本点来测试目标函数是，利用这个信息更新目标函数的先验分布
3. 算法测试由后验分布给出全局最优解可能出现的位置的点。



注意，如果贝叶斯优化找到一个局部最优解，会在该区域不断采样，所以很容易陷入局部最优解。为了解决这个问题，贝叶斯优化算法会在未取样的区域获取采样点，同时根据后验分布在最可能出现全局最值的区域进行采样。



### 2.7 Overfitting & Underfitting

**Overfitting 过拟合：**因为训练数据的原因，包括sampling、feature等原因，模型对于训练数据拟合过当，导致在训练集中表现很好，但是测试集和新数据上的表现较差。

**Underfitting 欠拟合：**模型在训练和预测时，表现都不好的情况。



#### 2.7.1 降低过拟合风险的方法

1. 增加更多数据，让模型学到更多更有效的特征。
2. 降低模型复杂度，高阶的函数形状更加复杂，采用更简单的模型（奥卡姆剃刀法则：在功能目标实现相同的情况下，选择更为简单的方法）。
3. 正则化，添加惩罚项penalty，对模型的参数进行约束。
4. 集成学习，如bagging，将多个模型集成在一起，来降低单一模型的过拟合风险。
5. dropout，训练的时候，随机丢掉一些结点和参数，验证的时候不drop掉

值得一提的是，bagging和dropout类似。

**正则化为什么可以防止过拟合**

- 令损失函数 L = R(y,f(x)
- 加入一个正则项，也就是惩罚项，通过这个正则项去控制参数W，也就是使W的个数N最小，目标函数从经验风险最小化转换到了结构风险最小化，即L = R(y,f(x)+P(w),然后总的目标是使L最小化
- 引入0，1，2范式，0范式是求非0的个数，1范式是绝对值之和，2范式是模；
- w=0是没有意义的，表示该x没有权重，所以采用0范式，然而0范式是很不好处理的，所以在某些情况下，用1范式代替0范式，0范式和1范式都能实现稀疏。

##### L1 和 L2（京东一面）

面试官答案：从数据方面来讲，L1正则化有特征选择的作用，因为L1正则会使特征变成0，对相互之间存在关系的特征来说，减少了特征。L2正则化因为是一个凸的形状，在优化过程中更加稳定。（是个很温柔的小姐姐）



L1 和 L2 的分布：

- L1属于拉普拉斯分布
- L2属于高斯分布

#### 2.7.2 降低欠拟合风险的方法

1. 添加新特征：当特征不足或者现有特征与样本标签的相关性不强，导致模型出现欠拟合。可以通过挖掘其他新的特征，往往可以取得更好的效果。
2. 增加模型复杂度
3. 减小正则化系数





## 3.传统机器学习

机器学习可以分为supervised learning, unsupervised learning, semi-supervised learning.

这里只介绍传统的机器学习算法。

**生成模型和判别模型：**

生成模型是求联合分布，无boundary的存在

判别模型是求条件概率，有boundary的存在



**To-Do:补充机器学习概念知识**





### 3.1 PLA（Perceptron Learning Algorithm 感知机学习算法）

**To-Do:补充**



### 3.2 SVM（Support Vector Machine 支持向量机）

SVM有三宝：间隔、对偶、核技巧。

SVM属于判别模型。

概念：SVM属于二分类模型，目的是寻找到一个超平面可以将样本进行划分，划分的依据是**间隔最大化**，最终将目标函数转化为一个**凸二次规划问题**来求解。

Support Vector支持向量：在求解过程中，只根据部分数据就可以确定分类器，这部分数据称为支持向量。所以支持向量是**距离超平面最近的点**。

**线性可分：**在数据集中，存在一个超平面，可以将数据集中的数据完全划分，则称为该数据集线性可分。

间隔可分为：

- Hard margin 硬间隔
- soft margin 软间隔

目标函数表达式：

硬间隔： $$ min_{w,b} \frac{1}{2} ||w||^2 \qquad st. \quad y^{(i)}(w^Tx^{(i)} + b) \geq 1 $$

软间隔： $$ min_{w,b} \frac{1}{2} ||w||^2 + C \sum_{i=1}^m \xi_i \qquad st. \quad y^{(i)}(w^Tx^{(i)} + b) \geq 1 ,\quad \xi_i \geq 0 $$



损失函数：Hinge Loss



SVM解决的问题：

- 线性分类：对于n维的数据，SVM的目标是找到一个n-1维的最佳超平面来讲数据划分成两部分。

  通过增加一个约束条件：支持向量到超平面的距离是最大的。

- 非线性分类：通过拉格朗日乘子法和KKT条件，结合核函数去对数据进行分类。



SVM的种类：

- 硬间隔 SVM：又称线性可分SVM
- 软间隔 SVM：部分数据点在错误的位置，允许有错误，且错误在可接受的范围。即**软间隔允许部分样本点不满足约束条件**。
- Kernel SVM：训练数据线性不可分，通过核技巧和软间隔最大化去学习非线性SVM



#### 几何间隔和函数间隔

二维空间中点 (x,y) 到超平面的距离公式：$$\frac {|Ax+By+C|}{\sqrt {A^2+B^2}}$$

点到超平面的距离公式：$$\frac {|w^Tx+b|} {||w||}$$





#### 3.2.1 推导

#### 3.2.2对偶问题

引入拉格朗日乘子法:

- 等式约束优化
- 不等式约束优化



- 弱对偶性： 最大的里面挑出来的最小的也要比最小的里面挑出来的最大的要大 $$ min , max , f \geq max , min , f $$
- 强对偶性：KKT 条件是强对偶性的充要条件。



#### 3.2.3 KKT条件

$$a_ig_i(x_i)=0$$

$$g_i(x_i)<=0$$

$$a_i >=0 $$





#### 3.2.4 松弛变量

解决当出现了outliers的情况。



#### 3.2.5 Kernel method核函数

对于在有限维度的向量空间中线性不可分的样本，将其映射到**更高维度**的向量空间，再通过间隔最大化的方法，学习得到SVM模型。

$$K(x_i,x_j)=\phi (x_i) \phi(x_j)$$



##### 核函数种类

- 线性核函数：主要用于线性可分的情形。参数少，速度快。
- 多项式核函数：
- 高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。
- sigmoid 核函数：
- 拉普拉斯核函数：

如果feature数量很大，跟样本数量差不多，建议使用LR或者 Linear kernel 的SVM。 如果 feature 数量较少，样本数量一般，建议使用 Gaussian Kernel 的SVM。



#### 3.2.6 SMO

**To-Do:补充**



#### LR 与 SVM 的区别？（京东一面）

- LR是参数模型，SVM是非参数模型
- 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
- SVM的处理方法是只考虑支持向量，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过**非线性映射**，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
- 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。
- logistic regression 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。
- 

### 3.3 决策树

自上而下，由结点和有向边组成。结点分为内部节点和叶子结点，内部节点表示一个特征或属性，子节点表示类别。

过程：

1. 特征选择
2. 树的构造
3. 树的剪枝

熵可以理解为信息的不确定性。

#### 3.3.1 ID3

最大信息增益，信息增益为信息的不确定性减小程度



缺点：

#### 3.3.2 C4.5

最大信息增益比



#### 3.3.3 CART (Classification and Regression Tree 分类回归树)

用最大基尼指数（Gini）去计算

Gini描述的是数据的纯度

$$Gini(D)=1-\sum_{k=1}^n (\frac {|C_k|}{|D|})^2$$

CART在每一次迭代中，选择**基尼指数最小**的特征，采用二院切割法，每一步根据特征A的取值切成两份，分别进入左右子树。

$$Gini(D\|A) = \sum_{i=1} \frac {|D_i|}{|D|} Gini(D_i)$$



#### 3.3.4 剪枝

##### 预剪枝

提前对树进行剪枝，比如当树的深度超过阈值，或分类达到某个标准，或选择的特征已达到选择的数量等。

但是这个需要先验知识才能知道什么时候或什么阈值下剪枝

##### 后剪枝

树的生成完成后再进行剪枝，开销比较大，同样可以根据阈值选择，但是也可以通过其他的后剪枝算法进行剪枝。



#### 3.3.5 Bagging & Boosting

##### Bagging(bootstrap aggregating)

思想就是从总体样本当中随机采样（有放回采样）进行训练，通过多次这样的结果，进行**投票法**或**取平均值**作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。

通过Bagging的结果variance和bias可能是小的。

在预测问题中，通常采用取平均值法，对几个模型的输出作一个加和平均；在分类问题中，通常采用投票法，把将投票最大的值作为输出。

Bagging的目的是降低Variance，当我们担心模型复杂度过高，很容易会出现过拟合的时候，采用bagging。

##### Boosting

与Bagging不一样，Bagging是用在很强的model上，而Boosting是用在比较弱的model上。

 架构：

1. 首先找到第一个分类器，
2. 然后要找到第二个分类器，与第一个分类器是**互补**的（互补的意思是不可以与第一个分类器相似）
3. 接着找到第三个分类区与第二个是互补的，一直将这个process进行下去
4. 最后把这些分类器集合起来就可以得到一个error rate很低的分类器。

> Boosting过程中，找到的模型是sequentially的，而Bagging中的是无序的



生成不同Classifier的方式是制造不同的training set的方式，包括：

- 采用不同的traing dataset
- Re-sampling
- Re-weighting：当分类错误的时候，给予一个新的权重($u*d_1$)，使该数据的权重提高；如果分类正确，则除以一个d($u/d_1 $)。即分类对了就减小权重，错误了就增加。然后下一个分类器就给予这个新的权重去进行训练
-  直接修改损失函数，添加一个权重

#### 3.3.6 Random Forest随机森林

基于树模型的Bagging的优化版本，多棵树生成，可以解决决策树泛化能力弱的问题。如果只是采用普通的树模型，并加上了sampling的方法，生成的所有树的结果都是相似的。所以引入了Random Forest，即在每次产生新的branch的时候，都会随机的决定feature，这样每次产生的树都是不一样的，最后把所有树的结果结合起来，就是输出结果。



##### Out-of-bag validation for bagging

传统的validation是将dataset切分成training dataset和test dataset，在Random Forest中，不需要将数据切分，只需要用生成树的时候没有用过的data去对模型进行test即可，最后对每个树的test的结果取平均。因此Out-of-bag error其实也是一个可以在Testing set上可以反映Testing set结果的estimation。



#### 3.3.7 Adaboost

结合Re-sampling 的权重更新方法：

算法：

遍历t次，生成t个分类器，初始化每个分类器的权重都是1

  $$u_{t+1}^n= \begin{cases}  
  u_t^n*d_t=u_t^n*exp(a_t) & misclassified \\
 u_t^n*d_t=u_t^n/exp(-a_t) & classified \\
  \end{cases}$$

将上式可写成 $u_{t+1}^n = u_t^n * exp(-y^n*f_t(x^n)a_t$

$$ d_t = \sqrt {\frac {1-error rate}{error rate}}$$

$$ a_t = \ln \sqrt {\frac {1-error rate}{error rate}}$$

得到了子树分类器后，将所有的分类器乘以不同的权重$a_t$并加起来，然后取符号，即$H(x) = sign(\sum_{t=1}^T a_tf_t(x))$。在训练的时候，错误率比较大的时候，weight是比较小的，如果错误率比较小的时候，weight就是比较大的。

**decision stump**：就是切刀，比如一个二维的空间，往某个维度切一刀，一边是+，另一半是-，这就是decision stump

** 注意，他是有一个upper bound 的**

如果在training set上的error rate是0的，说明这个algorithm是有问题的，但在testing data上仍然可以继续下降。





#### 3.3.7 GBDT(Gradient Boosting Decision Tree 梯度提升树)

GBDT需要将多棵树的得分累加得到最终的预测得分，且每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差。

是个前向优化算法，采用了泰勒公式去进行逼近



#### 3.3.8 XGBoost（京东一面）









### 3.4 LR（Logistic Regression 逻辑回归）

目标处理分类问题，而线性回归的回归预测问题，所以这是两者最本质的不同。

逻辑回归的输出是一个概率分布，即给定输入和模型的超参数后，得到的是输出的期望。

**本质：**极大似然估计

**激活函数：**sigmoid

**损失函数：**交叉熵cross-entropy

#### LR推导



#### 与线性回归的异同

#### LR处理多分类的方法

- Softmax Regression

- 一对一

  

### 3.5 朴素贝叶斯

为什么朴素？因为朴素贝叶斯算法假设的是每个**特征之间是相互独立**的。

- A:a random variable
- B:a random variable

#### Conditional Probability 条件概率

$$P(A|B) = \frac {P(AB)}{P(B)}$$

#### Bayes Rule

$$P(A|B) = \frac {P(B|A)P(A)}{P(B)}$$

#### 朴素贝叶斯分类器

给定一堆关于特征的数据record，去预测属于哪个Class，即：

$$maximizes P(C|A_1,A_1,...,A_n)$$

##### 实现方法：

1. 通过贝叶斯法则计算每个类别的后验概率$P(C|A_1,A_1,...,A_n)$

2. $$maximizes P(C|A_1,A_1,...,A_n) = \frac { P(A_1,A_1,...,A_n|C)P(C)}{P(A_1,A_1,...,A_n)}$$

   实际上，我们只关注$P(A_1,A_1,...,A_n|C)$这个部分，即这个部分最大即可

3. 所以假设特征之间相互独立，计算：$P(A_1,A_1,...,A_n|C) = P(A_1|C) P(A_2|C)... P(A_n|C)$





### 3.6 KNN

#### 时间复杂度

如果数据集的大小为N，每个数据的维度为D，则KNN算法的计算复杂度为O(D\*N\*N)



### 3.7 K-Mean

k-mean 的计算复杂度为O(NKt)，其中N代表样本的数目，K表示选择的簇数，t表示迭代的轮次

#### 3.7.1 计算过程

1. 首先对数据进行归一化和离群点处理
2. 然后人工选择K值
3. 初始化簇中心，随机选择K个点当做中心
4. 计算每个样本到每个簇的中心的距离，选择最近的那个点，将新样本加入
5. 重新计算每个簇的中心



#### k值与bias & variance 的关系（好几个面试和笔试都问了）

k值越大，bias越大，方差越小；k值越小，bias越小，方差越大



#### 缺点：

- 不知道最优的k值，且需要人工预设，K值与真实的数据分布未必吻合
- 只能收敛到局部最优
- 当初始化选择不佳，会导致Empty clusters或其他情况，效果受到初始值的影响很大
- 样本只能划分到单一的类中







改进优化方面：

1. K值的选择
   - 手肘法：递增遍历K，然后找到拐点，拐点所在位置就是最好的K值
   - K-means ++算法：初始化的时候选择距离最远的点作为初始点
2. 数据的处理

### 3.8 Hierarchical Clustering

### 3.9 DBSCAN





## 4. 数据挖掘相关算法

### Apriori

### FP-Tree



## 5. 优化方法

### 5.1 牛顿法

**基本思想：**在现有极小点估计值的附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值。



### 5.2 Gradient Descent

### 5.3 Momentum

### 5.4 AdaGrad

### 5.5 

### 5.6 Adam





## 6. 激活函数

### 6.1 ReLU

### 6.2 Sigmoid

### 6.3 Tanh

###