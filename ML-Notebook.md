# ML Notebook

## Feature Engineering 特征工程

### 特征选择

#### 如何选择特征？

基于业务理解，尽可能找出对因变量有影响的所有自变量。



#### 如何评估特征的可用性？

从三个方面去评估：特征数据的获取难度、覆盖率、准确率。



### 特征处理

未经过处理的特征可能会出现以下情况：

- 不同量级
- 离散型特征
- 存在缺失值
- 信息利用率低

#### 归一化

#### 离散型特征的处理方式

许多模型和算法都需要数值特征作为输入，因此需要将离散型的特征转换为数值特征。

离散型特征转换数值特征的方法：

- 序列编码
- one-hot编码
- 二进制编码



##### 序列编码

序列特征通常用来处理具有大小关系的特征。

如果特征中是可以被等级划分的，则可以划分成高中低等级，通过使用序列编码，将低等级用1表示，中等级用2表示，高等级用3表示。

##### one-hot编码

根据特征的数量，将每个特征编码成1*N维的向量，只有代表这个特征的维度表示为1，其他维度表示为0。

会具有**维度爆炸**的问题。

##### 二进制编码

将特征通过过序列标注，然后以二进制的形式，将特征序列号转换为二进制，这样可以减少空间。



#### 缺失值的处理

1. 通过平均值、中值、众数、随机值等替代。效果一般，因为这相当于人为添加了噪声。
2. 通过欧式距离或者Pearson相似度，来确定与缺失数据样本最近的K个样本， 将这K个样本的相关特征加权平均来估计该样本的缺失值。
3. 将变量映射到高维空间
   - 离散型变量：通过one-hot编码，映射成三个变量：男，女，缺失三个值向量
   - 连续性变量：进行分箱操作，采用一定的数据平滑方式（平均值/中值/箱边界）进行离散化，然后增加是否缺失这种维度。
4. 通过已有数据作线性回归，预测缺失值。

#### 离群值outliers的处理

因为过大或过小的数据可能会影响到分析结果，尤其是在做回归的时候，我们需要对那些离群值进行处理。

> 离群值和极值是不一样的，极值不一定会影响分析结果。

检测方法：

1. 基于标准差（Standard2Deviations, SD）法
   a为一组数的平均数，b为这组数的标准差，则超出[a-2b, a+2b]范围的值即被认为离群值。

   

   这种方法不是特别的靠谱，因为离群值的出现可能反过来很大程度影响平均数和标准差，所以平均数或者标准差受离群值的制约而使得这个检测方法不靠谱。

2. 基于绝对离差中位数（Median Absolute Deviation, MAD）的中心距离计算法
  （1）计算所有观察点的中位数median(X);
  （2）计算每个观察点与中位数的绝对偏差值abs(X-median(X));
  （3）计算（2）中的绝对偏差值的中位数，即MAD = median(abs(X - median(X)));
  （4）之后选定n值，从而确定合理的范围

  $$X_i^{'} = \begin{cases}  
  X_{median}+nMAD & X_i>X_{median}+nMAD \\
  X_{median}-nMAD & X_i<X_{median}-nMAD \\
  X_i & X_{median}-nMAD<X_i<X_{median}+nMAD
  \end{cases}$$

  这是一种稳健对抗离群数据的距离值方法，采用计算各观测值与平均值的距离总和的方法。放大了离群值的影响，相比基于SD的中位数距离法，它可以更清晰地从正常观察点中检测出离群值来。

3. 百分位法

   计算的逻辑是将因子值进行升序的排序，对排位百分位高于97.5%或排位百分位低于2.5%的因子值，进行类似于前两种方法的处理。

4. Box plot



#### 特征组合

有时候为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

##### 组合方法

- A*A
- A*B
- A\*B\*C*D

##### 处理高维特征

有时候，特征的数量过大（可以达到千万量级），这时候几乎是无法学习到这种规模的参数，因此可以采用矩阵分解的方法将特征用低维向量的方法表示。

##### 如何选择组合特征

可以采用GBDT（梯度提升决策树）的方法，每次都在之前构建的决策树的残差上构建下一棵决策树。



#### 特征选择

数据预处理结束之后，我们要选择有意义的特征去输入算法和模型进行训练，通常从以下两个方面来考虑：

- 是否发散
- 与目标的相关性



特征的选择方法可以分为Filter法（过滤法）、Wrapper法（包装法）、Embedded法（嵌入法）

##### Filter法-过滤法

按照发散性或者相关性，对各个特征进行评分，设定阈值或者选择阈值的个数，选择特征。

- 方差选择法：先计算各个特征的方差，选择方差大于阈值的特征。

- 相关系数法：先计算各个特征对目标值的相关系数P值。

- 卡方检验(Chi-square test)：检验定性自变量对定性因变量的相关性。$$x^2 = \sum \frac {(Observed - Expect)^2}{Expect}$$

  结合自由度、置信度查表分析。

  _reference-《结合日常生活的例子，了解什么是卡方检验》:https://www.jianshu.com/p/807b2c2bfd9b_

  

##### Wrapper法-包装法

根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。

- 递归特征消除法：使用一个及模型来进行多轮训练，每轮训练后，消除若干权值系数的特征

##### Embedded法-嵌入法

先试用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是通过训练来确定特征的优劣。

- 基于惩罚项的特征选择法：使用带有惩罚项的基模型，除了筛选出特征外，同时也进行了降维
- 基于树模型的特征选择法(GBDT)

#### 降维处理

当特征选择完成后，如果特征矩阵过大，会导致计算量大、训练时间长，所以为了有效、快速的训练模型，降低特征矩阵维度是一个有效的手段。



常见的降维方法除了基于L1惩罚项的模型外，还有以下两种方法：

- PCA（主成分分析）
- LDA（线性判别分析）

> 线性判别分析本身也是一个分类模型

PCA和LDA有很多共同点，本质都是要将原始的样本映射到维度更低的样本空间。但是两者的映射目标不一样，PCA是为了让银蛇后的样本具有最大发散性；而LDA是为了让映射后的样本有最好的分类性能。

所以，PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。